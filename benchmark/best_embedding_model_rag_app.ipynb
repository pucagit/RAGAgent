{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best open-source embedding model for your RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCE: \n",
    "- https://www.tigerdata.com/blog/finding-the-best-open-source-embedding-model-for-rag\n",
    "- https://huggingface.co/spaces/mteb/leaderboard\n",
    "  \n",
    "We evaluate 8 open-source embedding models: [nomic-embed-text](https://ollama.com/library/nomic-embed-text), [bge-m3](https://ollama.com/library/bge-m3/blobs/daec91ffb5dd), [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large/blobs/819c2adf5ce6), [qwen3-embedding:0.6b](https://ollama.com/library/qwen3-embedding:0.6b), [qwen3-embedding:4b](https://ollama.com/library/qwen3-embedding:4b), [embeddinggemma:300m](https://ollama.com/library/embeddinggemma:300m), [snowflake-arctic-embed:335m](https://ollama.com/library/snowflake-arctic-embed:335m), [granite-embedding:278m](https://ollama.com/library/granite-embedding:278m).\n",
    " \n",
    "To facilitate the execution of these models and the generation of embeddings, we will use the following tools:\n",
    "\n",
    "- [Ollama](https://ollama.com/), a platform that provides access to a variety of open-source Large Language Models (LLMs).\n",
    "- [pgai](https://github.com/timescale/pgai), an open-source extension that seamlessly integrates LLM workflows, such as embedding creation and management, directly into your PostgreSQL database.\n",
    "\n",
    "The evaluation process involves:\n",
    "\n",
    "1. Setting up a test environment with Ollama and PostgreSQL\n",
    "2. Loading Paul Graham's essays as our test dataset\n",
    "3. Generating embeddings using different models\n",
    "4. Creating diverse test questions across multiple categories\n",
    "5. Evaluating each model's retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before you begin, follow the steps in [pgai-quick-start.md](pgai-quick-start.md) to setup pgai and download the suitable embedding models.\n",
    "\n",
    "Let's install the necessary Python libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: psycopg2-binary in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: Jinja2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (3.1.6)\n",
      "Requirement already satisfied: pgai in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: vincent in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (0.4.4)\n",
      "Requirement already satisfied: XlsxWriter in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (3.2.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from Jinja2) (3.0.2)\n",
      "Requirement already satisfied: click<9.0,>=8.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (8.3.0)\n",
      "Requirement already satisfied: datadog-lambda<7.0,>=6.9 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (6.111.0)\n",
      "Requirement already satisfied: psycopg<4.0,>=3.2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from psycopg[binary]<4.0,>=3.2->pgai) (3.2.10)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (2.11.9)\n",
      "Requirement already satisfied: python-dotenv<2.0,>=1.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (1.1.1)\n",
      "Requirement already satisfied: pytimeparse<2.0,>=1.1 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (1.1.8)\n",
      "Requirement already satisfied: semver>=3.0.4 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (3.0.4)\n",
      "Requirement already satisfied: structlog<26.0,>=24.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (25.4.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pgai) (4.15.0)\n",
      "Requirement already satisfied: colorama in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from click<9.0,>=8.0->pgai) (0.4.6)\n",
      "Requirement already satisfied: datadog<1.0.0,>=0.51.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from datadog-lambda<7.0,>=6.9->pgai) (0.52.1)\n",
      "Requirement already satisfied: ddtrace<4,>=2.20.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from datadog-lambda<7.0,>=6.9->pgai) (3.15.0)\n",
      "Requirement already satisfied: ujson>=5.9.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from datadog-lambda<7.0,>=6.9->pgai) (5.11.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.11.2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from datadog-lambda<7.0,>=6.9->pgai) (1.17.3)\n",
      "Requirement already satisfied: requests>=2.6.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from datadog<1.0.0,>=0.51.0->datadog-lambda<7.0,>=6.9->pgai) (2.32.5)\n",
      "Requirement already satisfied: bytecode>=0.16.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (0.17.0)\n",
      "Requirement already satisfied: envier~=0.6.1 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (0.6.1)\n",
      "Requirement already satisfied: legacy-cgi>=2.0.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (2.6.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (1.37.0)\n",
      "Requirement already satisfied: protobuf>=3 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (5.29.5)\n",
      "Requirement already satisfied: psycopg-binary==3.2.10 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from psycopg[binary]<4.0,>=3.2->pgai) (3.2.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pydantic<3.0,>=2.0->pgai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pydantic<3.0,>=2.0->pgai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from pydantic<3.0,>=2.0->pgai) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from opentelemetry-api>=1->ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1->ddtrace<4,>=2.20.0->datadog-lambda<7.0,>=6.9->pgai) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from requests>=2.6.0->datadog<1.0.0,>=0.51.0->datadog-lambda<7.0,>=6.9->pgai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from requests>=2.6.0->datadog<1.0.0,>=0.51.0->datadog-lambda<7.0,>=6.9->pgai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from requests>=2.6.0->datadog<1.0.0,>=0.51.0->datadog-lambda<7.0,>=6.9->pgai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ai-llm\\agents\\ragagent\\.venv\\lib\\site-packages (from requests>=2.6.0->datadog<1.0.0,>=0.51.0->datadog-lambda<7.0,>=6.9->pgai) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas psycopg2-binary Jinja2 pgai vincent XlsxWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_CONNECTION_STRING=\"postgres://postgres:postgres@localhost:5432/postgres\"\n",
    "OLLAMA_HOST=\"http://ollama:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Ingestion\n",
    "\n",
    "Let's setup the PostgreSQL database and install [pgai](https://github.com/timescale/pgai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def connect_db():\n",
    "    return psycopg2.connect(DATABASE_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_db() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS ai CASCADE;\")\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS cyber_attack (\n",
    "                id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n",
    "                title TEXT NOT NULL,\n",
    "                category TEXT,\n",
    "                attack_type TEXT,\n",
    "                scenario_description TEXT,\n",
    "                tools_used TEXT,\n",
    "                attack_steps TEXT,\n",
    "                target_type TEXT,\n",
    "                vulnerability TEXT, \n",
    "                mitre_technique TEXT,\n",
    "                impact TEXT,\n",
    "                detection_method TEXT,\n",
    "                solution TEXT,\n",
    "                tags TEXT,\n",
    "                source TEXT\n",
    "            );\n",
    "          \"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We'll now load [Cybersecurity Attack dataset](https://www.kaggle.com/datasets/tannubarot/cybersecurity-attack-and-defence-dataset?resource=download) into our PostgreSQL database (since the code only pulls dataset from Huggingface, we need to install the dataset from Kaggle and upload it to Huggingface). The dataset consists of 14133 cyber attacks with title, category, attack type, scenario description, tools used, attack steps, target type, vulnerability, MITRE technique, impact, detection method, solution, tags, source. We'll verify the successful ingestion by displaying the first cyber attack from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Authentication Bypass via SQL Injection', ' Mobile Security', 'SQL Injection (SQLi)', 'A login form fails to validate or sanitize input, allowing attackers to log in as admin without knowing the password.', 'Browser, Burp Suite, SQLMap', \"1. Reconnaissance: Find a login form on the website (e.g., username and password fields). 2. Test for Injection: Enter a simple payload like ' OR '1'='1 in the username or password field. 3. Analyze Response: If the login succeeds or error message changes, it may be vulnerable. 4. Craft Exploit: Use payloads like: Username: ' OR '1'='1' --  Password: anything. 5. Bypass Authentication: The SQL query behind the scenes becomes: SELECT * FROM users WHERE username='' OR '1'='1' -- ' AND password='anything'. This always returns true (1=1), tricking the system to log in without a password.. 6. Access Granted: Attacker gets access to admin or user accounts.\", 'Web Login Portals (e.g., banking, admin dashboards, e-commerce)', 'Unsanitized input fields in SQL queries', 'T1078 (Valid Accounts), T1190 (Exploit Public-Facing App)', 'Full account takeover, data theft, privilege escalation', 'Web server logs, anomaly detection (e.g., logins without passwords), WAF alerts', 'Use prepared statements, Sanitize inputs, Limit login attempts, Use CAPTCHA, Enable MFA', 'SQLi, Authentication Bypass, Web Security, OWASP Top 10', 'OWASP, MITRE ATT&CK, DVWA')\n"
     ]
    }
   ],
   "source": [
    "with connect_db() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Load Cybersecurity Attack dataset into the 'cyber_attack' table\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT ai.load_dataset(\n",
    "                    'pucavv/Cybersecurity_Attack', \n",
    "                    table_name => 'cyber_attack', \n",
    "                    if_table_exists => 'append');\n",
    "        \"\"\")\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        # Fetch and print the first row from the 'cyber_attack' table to verify the data\n",
    "        cur.execute(\"SELECT * FROM cyber_attack LIMIT 1;\")\n",
    "        print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "pgai makes generating embeddings for various models incredibly straightforward with its [pgai Vectorizer](https://github.com/timescale/pgai/blob/main/docs/vectorizer.md). With just a single SQL command, you can effortlessly configure a vectorizer to automatically create and update embeddings from any chosen embedding model.\n",
    "\n",
    "We create vectorizers for each embedding model. Each vectorizer will:\n",
    "\n",
    "- Process text using the same chunking strategy (512 characters with 50 character overlap)\n",
    "- Use consistent text formatting across all models\n",
    "- Generate embeddings with model-specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(embedding_model, embeddings_dimensions):\n",
    "    embeddings_view_name = f\"{'cyber_attack'}{'_'}{embedding_model.replace('-','_').replace(':','_').replace('.','_')}{'_'}{'emb'}\"\n",
    "\n",
    "    with connect_db() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT ai.create_vectorizer(\n",
    "                'cyber_attack'::regclass,\n",
    "                destination => ai.destination_table(%s),\n",
    "                loading => ai.loading_column('scenario_description'),\n",
    "                embedding => ai.embedding_ollama(%s, %s),\n",
    "                chunking => ai.chunking_recursive_character_text_splitter(512, 50),\n",
    "                formatting => ai.formatting_python_template('title: $title $chunk')\n",
    "            );\n",
    "            \"\"\", (embeddings_view_name, embedding_model, embeddings_dimensions, )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = [\n",
    "    {'name':'mxbai-embed-large', 'dimensions': 1024},\n",
    "    {'name':'nomic-embed-text','dimensions': 768},\n",
    "    {'name':'bge-m3','dimensions': 1024},\n",
    "    {'name':'qwen3-embedding:0.6b', 'dimensions': 1024},\n",
    "    {'name':'qwen3-embedding:4b','dimensions': 2560},\n",
    "    {'name':'embeddinggemma:300m','dimensions': 768},\n",
    "    {'name':'snowflake-arctic-embed:335m', 'dimensions': 1024},\n",
    "    {'name':'granite-embedding:278m','dimensions': 768},\n",
    "] \n",
    "\n",
    "for model in EMBEDDING_MODELS:\n",
    "    create_vectorizer(model['name'], model['dimensions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizers will take some time to complete the embedding generation. Use the following command to monitor their progress:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer ID: 1, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_mxbai_embed_large_emb (0 chunks)\n",
      "Vectorizer ID: 2, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_nomic_embed_text_emb (0 chunks)\n",
      "Vectorizer ID: 3, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_bge_m3_emb (0 chunks)\n",
      "Vectorizer ID: 4, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_qwen3_embedding_0_6b_emb (0 chunks)\n",
      "Vectorizer ID: 5, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_qwen3_embedding_4b_emb (0 chunks)\n",
      "Vectorizer ID: 6, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_embeddinggemma_300m_emb (0 chunks)\n",
      "Vectorizer ID: 7, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_snowflake_arctic_embed_335m_emb (0 chunks)\n",
      "Vectorizer ID: 8, Embedding Table: public.cyber_attack, Pending Items: public.cyber_attack_granite_embedding_278m_emb (0 chunks)\n"
     ]
    }
   ],
   "source": [
    "with connect_db() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT * FROM ai.vectorizer_status;\")\n",
    "\n",
    "        for row in cur.fetchall():\n",
    "            print(f\"Vectorizer ID: {row[0]}, Embedding Table: {row[2]}, Pending Items: {row[4]} ({row[6]} chunks)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this inside the pgai-db-1 container to delete duplicate constraints:\n",
    "```\n",
    "DELETE FROM ai.vectorizer WHERE name = '<duplicate_table>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Parameters\n",
    "\n",
    "To ensure a fair comparison, we'll establish consistent evaluation parameters:\n",
    "\n",
    "- Number of text chunks to evaluate\n",
    "- Questions per chunk across different categories\n",
    "- Number of top results to consider (K)\n",
    "- Distribution of question types to test different aspects of embedding quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS = 20\n",
    "NUM_QUESTIONS_PER_CHUNK = 20\n",
    "TOP_K = 10\n",
    "\n",
    "QUESTION_DISTRIBUTION = {\n",
    "    'short': 4,\n",
    "    'long': 4,\n",
    "    'direct': 4,\n",
    "    'implied': 4,\n",
    "    'unclear': 4\n",
    "}\n",
    "\n",
    "assert sum(QUESTION_DISTRIBUTION.values()) == NUM_QUESTIONS_PER_CHUNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Chunks\n",
    "\n",
    "We select 20 random chunks from one of the embeddings view since all models use the same chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "evaluation_chunks = []\n",
    "\n",
    "with connect_db() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "                SELECT id, chunk_seq, chunk, embedding \n",
    "                FROM cyber_attack_nomic_embed_text_emb_store \n",
    "                ORDER BY RANDOM() \n",
    "                LIMIT %s\n",
    "            \"\"\", (NUM_CHUNKS,))\n",
    "        \n",
    "        for row in cur.fetchall():\n",
    "            evaluation_chunks.append({\n",
    "                'id': row[0],\n",
    "                'chunk_seq': row[1],\n",
    "                'chunk': row[2],\n",
    "                'embedding': row[3]\n",
    "            })\n",
    "\n",
    "pd.DataFrame(evaluation_chunks).to_csv('./evaluation_data/chunks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Question Generation\n",
    "\n",
    "We'll generate diverse questions for each text chunk across five categories:\n",
    "\n",
    "1. **Short questions**: Simple, direct queries under 10 words\n",
    "2. **Long questions**: Detailed questions requiring comprehensive understanding\n",
    "3. **Direct questions**: Questions about explicit information\n",
    "4. **Implied questions**: Questions requiring contextual understanding\n",
    "5. **Unclear questions**: Ambiguous queries to test robustness\n",
    "\n",
    "Each category tests different aspects of the embedding models' capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_by_question_type(chunk, question_type, num_questions):\n",
    "    prompts = {\n",
    "        'short': \"Generate {count} short, simple questions about this text. Questions should be direct, under 10 words\",\n",
    "        'long': \"Generate {count} detailed, comprehensive questions about this text. Include specific details:\",\n",
    "        'direct': \"Generate {count} questions that directly ask about explicit information in this text\",\n",
    "        'implied': \"Generate {count} questions that require understanding context and implications of the text:\",\n",
    "        'unclear': \"Generate {count} vague, ambiguous questions about the general topic of this text:\"\n",
    "    }\n",
    "\n",
    "    prompt = prompts[question_type].format(count=num_questions) + f\"\\n\\nText: {chunk}\"\n",
    "\n",
    "    system_instructions = \"\"\"\n",
    "        Generate different types of questions about the given text following the prompt provided. \n",
    "        Each question must be on a new line. Do not include empty lines or blank questions. ONLY output the questions, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    with connect_db() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT ai.ollama_generate(\n",
    "                    'llama3.1:8b-instruct-q4_0',\n",
    "                    %s,\n",
    "                    system_prompt=>%s, \n",
    "                    host=>%s\n",
    "                )->>'response';\n",
    "            \"\"\",(prompt, system_instructions, OLLAMA_HOST))\n",
    "\n",
    "            generated_questions = [q.strip() for q in cur.fetchone()[0].split(\"\\n\") if q.strip()] # type: ignore\n",
    "            print(f\"Number of questions generated for {question_type}: {len(generated_questions)}\")\n",
    "            return generated_questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 2/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 3/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 4/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 5/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 6/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 7/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 8/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 9/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 5\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 10/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 11/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 12/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 13/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 14/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 15/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 16/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 17/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 7\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 18/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 19/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 2\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Processing chunk 20/20\n",
      "Number of questions generated for short: 4\n",
      "Number of questions generated for long: 4\n",
      "Number of questions generated for direct: 4\n",
      "Number of questions generated for implied: 4\n",
      "Number of questions generated for unclear: 4\n",
      "Generated questions in total: 402\n"
     ]
    }
   ],
   "source": [
    "evaluation_questions = []\n",
    "\n",
    "for i, chunk in enumerate(evaluation_chunks, 1):\n",
    "    print(f\"Processing chunk {i}/{len(evaluation_chunks)}\")\n",
    "\n",
    "    for question_type, count in QUESTION_DISTRIBUTION.items():\n",
    "        questions = generate_questions_by_question_type(chunk['chunk'], question_type, count)\n",
    "\n",
    "        for q in questions:\n",
    "            evaluation_questions.append({\n",
    "                'question': q,\n",
    "                'source_chunk_id': chunk['id'],\n",
    "                'source_chunk_seq': chunk['chunk_seq'],\n",
    "                'question_type': question_type,\n",
    "                'chunk': chunk['chunk']\n",
    "            })\n",
    "\n",
    "print(\"Generated questions in total:\", len(evaluation_questions))\n",
    "\n",
    "pd.DataFrame(evaluation_questions).to_csv('./evaluation_data/generated_questions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Evaluation\n",
    "\n",
    "The evaluation process involves:\n",
    "\n",
    "1. For each model:\n",
    "\n",
    "   - Converting questions to embeddings\n",
    "   - Performing similarity search against chunk embeddings\n",
    "   - Checking if the source chunk appears in `top-K` results\n",
    "\n",
    "2. Calculating performance metrics:\n",
    "\n",
    "   - Overall accuracy\n",
    "   - Per-question-type accuracy\n",
    "   - Detailed success/failure analysis\n",
    "\n",
    "This comprehensive evaluation will help identify each model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# OLLAMA_HOST = os.environ[\"OLLAMA_HOST\"]\n",
    "\n",
    "def vector_similarity_search(embeddings_view, embedding_model, question):\n",
    "    with connect_db() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT id, chunk_seq \n",
    "                FROM {embeddings_view} \n",
    "                ORDER BY embedding <=> ai.ollama_embed(%s, %s, host => %s)\n",
    "                LIMIT %s;\n",
    "            \"\"\", (embedding_model, question, OLLAMA_HOST, TOP_K,)\n",
    "            )\n",
    "\n",
    "            return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embedding_models():\n",
    "    evaluation_results = []\n",
    "    detailed_results = []\n",
    "\n",
    "    for model in EMBEDDING_MODELS:\n",
    "        print(f\"Evaluating {model['name']}...\")\n",
    "\n",
    "        embeddings_view = f\"{'cyber_attack'}{'_'}{model['name'].replace('-','_').replace(':','_').replace('.','_')}{'_'}{'emb'}\"\n",
    "        scores = []\n",
    "\n",
    "        for q in evaluation_questions:\n",
    "            vector_search_results = vector_similarity_search(embeddings_view, model['name'], q['question'])\n",
    "            found = any(\n",
    "                row[0] == q['source_chunk_id'] and row[1]== q['source_chunk_seq'] \n",
    "                for row in vector_search_results\n",
    "            )\n",
    "\n",
    "            scores.append(1 if found else 0)\n",
    "\n",
    "            detailed_results.append({\n",
    "                'model': model['name'],\n",
    "                'question': q['question'],\n",
    "                'question_type': q['question_type'],\n",
    "                'source_chunk_id': q['source_chunk_id'],\n",
    "                'source_chunk_seq': q['source_chunk_seq'],\n",
    "                'found_correct_chunk': found,\n",
    "                'num_results': len(vector_search_results)\n",
    "            })\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'model': model['name'],\n",
    "            'overall_accuracy': sum(scores) / len(scores),\n",
    "            'by_type': {\n",
    "                q_type: sum(scores[i] for i, q in enumerate(evaluation_questions) \n",
    "                            if q['question_type'] == q_type) / QUESTION_DISTRIBUTION[q_type] / NUM_CHUNKS\n",
    "                for q_type in QUESTION_DISTRIBUTION.keys()\n",
    "            }\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(detailed_results).to_csv('./evaluation_data/detailed_results.csv')\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating mxbai-embed-large...\n",
      "Evaluating nomic-embed-text...\n",
      "Evaluating bge-m3...\n",
      "Evaluating qwen3-embedding:0.6b...\n",
      "Evaluating qwen3-embedding:4b...\n",
      "Evaluating embeddinggemma:300m...\n",
      "Evaluating snowflake-arctic-embed:335m...\n",
      "Evaluating granite-embedding:278m...\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "evaluation_results = evaluate_embedding_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(evaluation_results):\n",
    "    # Create lists to store the data\n",
    "    rows = []\n",
    "    \n",
    "    # Process each model's results\n",
    "    for result in evaluation_results:\n",
    "        row = {\n",
    "            'Model': result['model'],\n",
    "            'Overall Accuracy': f\"{result['overall_accuracy']:.2%}\",\n",
    "        }\n",
    "        # Add accuracies for each question type\n",
    "        for q_type, acc in result['by_type'].items():\n",
    "            row[q_type.capitalize()] = f\"{acc:.2%}\"\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Reorder columns to put Overall Accuracy after Model\n",
    "    columns = ['Model', 'Overall Accuracy'] + [col for col in df.columns if col not in ['Model', 'Overall Accuracy']]\n",
    "    df = df[columns]\n",
    "    \n",
    "    # Display the table\n",
    "    return df.style.set_properties(**{\n",
    "        'text-align': 'center',\n",
    "        'border': '1px solid black',\n",
    "        'padding': '8px'\n",
    "    }).set_table_styles([ # type: ignore\n",
    "        {'selector': 'th', 'props': [\n",
    "            ('background-color', 'black'),\n",
    "            ('text-align', 'center'),\n",
    "            ('padding', '8px'),\n",
    "            ('border', '1px solid black')\n",
    "        ]},\n",
    "        {'selector': 'caption', 'props': [\n",
    "            ('text-align', 'center'),\n",
    "            ('font-weight', 'bold'),\n",
    "            ('font-size', '1.1em'),\n",
    "            ('padding', '8px')\n",
    "        ]}\n",
    "    ]).set_caption('Embedding Models Evaluation Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1c226 th {\n",
       "  background-color: black;\n",
       "  text-align: center;\n",
       "  padding: 8px;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_1c226 caption {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "  font-size: 1.1em;\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_1c226_row0_col0, #T_1c226_row0_col1, #T_1c226_row0_col2, #T_1c226_row0_col3, #T_1c226_row0_col4, #T_1c226_row0_col5, #T_1c226_row0_col6, #T_1c226_row1_col0, #T_1c226_row1_col1, #T_1c226_row1_col2, #T_1c226_row1_col3, #T_1c226_row1_col4, #T_1c226_row1_col5, #T_1c226_row1_col6, #T_1c226_row2_col0, #T_1c226_row2_col1, #T_1c226_row2_col2, #T_1c226_row2_col3, #T_1c226_row2_col4, #T_1c226_row2_col5, #T_1c226_row2_col6, #T_1c226_row3_col0, #T_1c226_row3_col1, #T_1c226_row3_col2, #T_1c226_row3_col3, #T_1c226_row3_col4, #T_1c226_row3_col5, #T_1c226_row3_col6, #T_1c226_row4_col0, #T_1c226_row4_col1, #T_1c226_row4_col2, #T_1c226_row4_col3, #T_1c226_row4_col4, #T_1c226_row4_col5, #T_1c226_row4_col6, #T_1c226_row5_col0, #T_1c226_row5_col1, #T_1c226_row5_col2, #T_1c226_row5_col3, #T_1c226_row5_col4, #T_1c226_row5_col5, #T_1c226_row5_col6, #T_1c226_row6_col0, #T_1c226_row6_col1, #T_1c226_row6_col2, #T_1c226_row6_col3, #T_1c226_row6_col4, #T_1c226_row6_col5, #T_1c226_row6_col6, #T_1c226_row7_col0, #T_1c226_row7_col1, #T_1c226_row7_col2, #T_1c226_row7_col3, #T_1c226_row7_col4, #T_1c226_row7_col5, #T_1c226_row7_col6 {\n",
       "  text-align: center;\n",
       "  border: 1px solid black;\n",
       "  padding: 8px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1c226\">\n",
       "  <caption>Embedding Models Evaluation Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1c226_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_1c226_level0_col1\" class=\"col_heading level0 col1\" >Overall Accuracy</th>\n",
       "      <th id=\"T_1c226_level0_col2\" class=\"col_heading level0 col2\" >Short</th>\n",
       "      <th id=\"T_1c226_level0_col3\" class=\"col_heading level0 col3\" >Long</th>\n",
       "      <th id=\"T_1c226_level0_col4\" class=\"col_heading level0 col4\" >Direct</th>\n",
       "      <th id=\"T_1c226_level0_col5\" class=\"col_heading level0 col5\" >Implied</th>\n",
       "      <th id=\"T_1c226_level0_col6\" class=\"col_heading level0 col6\" >Unclear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1c226_row0_col0\" class=\"data row0 col0\" >mxbai-embed-large</td>\n",
       "      <td id=\"T_1c226_row0_col1\" class=\"data row0 col1\" >57.96%</td>\n",
       "      <td id=\"T_1c226_row0_col2\" class=\"data row0 col2\" >45.00%</td>\n",
       "      <td id=\"T_1c226_row0_col3\" class=\"data row0 col3\" >93.75%</td>\n",
       "      <td id=\"T_1c226_row0_col4\" class=\"data row0 col4\" >53.75%</td>\n",
       "      <td id=\"T_1c226_row0_col5\" class=\"data row0 col5\" >73.75%</td>\n",
       "      <td id=\"T_1c226_row0_col6\" class=\"data row0 col6\" >25.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1c226_row1_col0\" class=\"data row1 col0\" >nomic-embed-text</td>\n",
       "      <td id=\"T_1c226_row1_col1\" class=\"data row1 col1\" >59.20%</td>\n",
       "      <td id=\"T_1c226_row1_col2\" class=\"data row1 col2\" >43.75%</td>\n",
       "      <td id=\"T_1c226_row1_col3\" class=\"data row1 col3\" >95.00%</td>\n",
       "      <td id=\"T_1c226_row1_col4\" class=\"data row1 col4\" >55.00%</td>\n",
       "      <td id=\"T_1c226_row1_col5\" class=\"data row1 col5\" >73.75%</td>\n",
       "      <td id=\"T_1c226_row1_col6\" class=\"data row1 col6\" >30.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1c226_row2_col0\" class=\"data row2 col0\" >bge-m3</td>\n",
       "      <td id=\"T_1c226_row2_col1\" class=\"data row2 col1\" >63.93%</td>\n",
       "      <td id=\"T_1c226_row2_col2\" class=\"data row2 col2\" >55.00%</td>\n",
       "      <td id=\"T_1c226_row2_col3\" class=\"data row2 col3\" >95.00%</td>\n",
       "      <td id=\"T_1c226_row2_col4\" class=\"data row2 col4\" >61.25%</td>\n",
       "      <td id=\"T_1c226_row2_col5\" class=\"data row2 col5\" >80.00%</td>\n",
       "      <td id=\"T_1c226_row2_col6\" class=\"data row2 col6\" >30.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1c226_row3_col0\" class=\"data row3 col0\" >qwen3-embedding:0.6b</td>\n",
       "      <td id=\"T_1c226_row3_col1\" class=\"data row3 col1\" >53.48%</td>\n",
       "      <td id=\"T_1c226_row3_col2\" class=\"data row3 col2\" >28.75%</td>\n",
       "      <td id=\"T_1c226_row3_col3\" class=\"data row3 col3\" >91.25%</td>\n",
       "      <td id=\"T_1c226_row3_col4\" class=\"data row3 col4\" >50.00%</td>\n",
       "      <td id=\"T_1c226_row3_col5\" class=\"data row3 col5\" >75.00%</td>\n",
       "      <td id=\"T_1c226_row3_col6\" class=\"data row3 col6\" >23.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1c226_row4_col0\" class=\"data row4 col0\" >qwen3-embedding:4b</td>\n",
       "      <td id=\"T_1c226_row4_col1\" class=\"data row4 col1\" >57.46%</td>\n",
       "      <td id=\"T_1c226_row4_col2\" class=\"data row4 col2\" >37.50%</td>\n",
       "      <td id=\"T_1c226_row4_col3\" class=\"data row4 col3\" >96.25%</td>\n",
       "      <td id=\"T_1c226_row4_col4\" class=\"data row4 col4\" >55.00%</td>\n",
       "      <td id=\"T_1c226_row4_col5\" class=\"data row4 col5\" >77.50%</td>\n",
       "      <td id=\"T_1c226_row4_col6\" class=\"data row4 col6\" >22.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1c226_row5_col0\" class=\"data row5 col0\" >embeddinggemma:300m</td>\n",
       "      <td id=\"T_1c226_row5_col1\" class=\"data row5 col1\" >29.60%</td>\n",
       "      <td id=\"T_1c226_row5_col2\" class=\"data row5 col2\" >7.50%</td>\n",
       "      <td id=\"T_1c226_row5_col3\" class=\"data row5 col3\" >68.75%</td>\n",
       "      <td id=\"T_1c226_row5_col4\" class=\"data row5 col4\" >28.75%</td>\n",
       "      <td id=\"T_1c226_row5_col5\" class=\"data row5 col5\" >36.25%</td>\n",
       "      <td id=\"T_1c226_row5_col6\" class=\"data row5 col6\" >7.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1c226_row6_col0\" class=\"data row6 col0\" >snowflake-arctic-embed:335m</td>\n",
       "      <td id=\"T_1c226_row6_col1\" class=\"data row6 col1\" >38.06%</td>\n",
       "      <td id=\"T_1c226_row6_col2\" class=\"data row6 col2\" >30.00%</td>\n",
       "      <td id=\"T_1c226_row6_col3\" class=\"data row6 col3\" >68.75%</td>\n",
       "      <td id=\"T_1c226_row6_col4\" class=\"data row6 col4\" >35.00%</td>\n",
       "      <td id=\"T_1c226_row6_col5\" class=\"data row6 col5\" >46.25%</td>\n",
       "      <td id=\"T_1c226_row6_col6\" class=\"data row6 col6\" >11.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c226_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1c226_row7_col0\" class=\"data row7 col0\" >granite-embedding:278m</td>\n",
       "      <td id=\"T_1c226_row7_col1\" class=\"data row7 col1\" >59.70%</td>\n",
       "      <td id=\"T_1c226_row7_col2\" class=\"data row7 col2\" >48.75%</td>\n",
       "      <td id=\"T_1c226_row7_col3\" class=\"data row7 col3\" >93.75%</td>\n",
       "      <td id=\"T_1c226_row7_col4\" class=\"data row7 col4\" >52.50%</td>\n",
       "      <td id=\"T_1c226_row7_col5\" class=\"data row7 col5\" >77.50%</td>\n",
       "      <td id=\"T_1c226_row7_col6\" class=\"data row7 col6\" >27.50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18d0cc3ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_table = create_results_table(evaluation_results)\n",
    "display(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vincent.colors import brews\n",
    "\n",
    "def create_grouped_column_chart(evaluation_results, excel_file='./evaluation_data/grouped_column.xlsx', sheet_name='Results'):\n",
    "    rows = []\n",
    "\n",
    "    # Process each model's results\n",
    "    for result in evaluation_results:\n",
    "        row = {\n",
    "            'Model': result['model'],\n",
    "            'Overall Accuracy': result['overall_accuracy'] * 100,\n",
    "        }\n",
    "        # Add accuracies for each question type\n",
    "        for q_type, acc in result['by_type'].items():\n",
    "            row[q_type.capitalize()] = acc * 100\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns to put Overall Accuracy after Model\n",
    "    columns = ['Model', 'Overall Accuracy'] + [col for col in df.columns if col not in ['Model', 'Overall Accuracy']]\n",
    "    df = df[columns]\n",
    "\n",
    "    # ===== Ghi ra Excel + Tạo biểu đồ =====\n",
    "    with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "        workbook  = writer.book\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        chart = workbook.add_chart({'type': 'column'}) # type: ignore\n",
    "\n",
    "        start_row = 1\n",
    "        end_row   = len(df)\n",
    "        categories = [sheet_name, start_row, 0, start_row + end_row - 1, 0]  # cột A (Model)\n",
    "\n",
    "        # Vẽ series cho từng cột từ \"Overall Accuracy\" đến \"Unclear\"\n",
    "        for i, col_name in enumerate(columns, start=1):\n",
    "            chart.add_series({\n",
    "                'name':       [sheet_name, 0, i],\n",
    "                'categories': categories,\n",
    "                'values':     [sheet_name, start_row, i, start_row + end_row - 1, i],\n",
    "                'fill':       {'color': brews['RdPu'][len(columns) - i]},\n",
    "                'gap':        150,\n",
    "            })\n",
    "\n",
    "        chart.set_title({'name': 'Embedding Models — Grouped Column (Overall + Question Types)'})\n",
    "        chart.set_x_axis({'name': 'Models'})\n",
    "        chart.set_y_axis({'name': 'Accuracy (%)', 'major_gridlines': {'visible': False}})\n",
    "        chart.set_legend({'position': 'bottom'})\n",
    "        chart.set_size({'width': 950, 'height': 520})\n",
    "\n",
    "        worksheet.insert_chart('I2', chart)\n",
    "\n",
    "    print(f\"✅ File Excel đã tạo: {excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File Excel đã tạo: ./evaluation_data/grouped_column.xlsx\n"
     ]
    }
   ],
   "source": [
    "create_grouped_column_chart(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragagent (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
