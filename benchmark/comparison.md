# **CHÆ¯Æ NG 4: Káº¾T QUáº¢ THá»°C NGHIá»†M**

## **1. Má»¥c tiÃªu**

Thá»±c nghiá»‡m nháº±m **Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng truy há»“i (retrieval performance)** cá»§a cÃ¡c mÃ´ hÃ¬nh embedding mÃ£ nguá»“n má»Ÿ trong há»‡ thá»‘ng **Retrieval-Augmented Generation (RAG)**, táº­p trung vÃ o:

* So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c giá»¯a cÃ¡c mÃ´ hÃ¬nh theo loáº¡i cÃ¢u há»i;
* PhÃ¢n tÃ­ch Ä‘áº·c tÃ­nh ká»¹ thuáº­t áº£nh hÆ°á»Ÿng Ä‘áº¿n kháº£ nÄƒng biá»ƒu diá»…n ngá»¯ nghÄ©a;
* Lá»±a chá»n mÃ´ hÃ¬nh phÃ¹ há»£p cho mÃ´i trÆ°á»ng RAG cá»¥c bá»™ trong lÄ©nh vá»±c báº£o máº­t.

---

## **2. Thiáº¿t láº­p**

* **Dataset:** *Paul Grahamâ€™s Essays* (~6.000 Ä‘oáº¡n, má»—i Ä‘oáº¡n ~512 kÃ½ tá»±).
* **CÃ´ng cá»¥:** PostgreSQL + `pgai Vectorizer`, cháº¡y embedding qua Ollama.
* **CÃ¡c mÃ´ hÃ¬nh thá»­ nghiá»‡m:** 8 mÃ´ hÃ¬nh embedding mÃ£ nguá»“n má»Ÿ.
* **PhÃ¢n loáº¡i cÃ¢u há»i:**

  1. *Short* â€“ cÃ¢u ngáº¯n, rÃµ rÃ ng
  2. *Long* â€“ cÃ¢u dÃ i, cÃ³ ngá»¯ cáº£nh
  3. *Direct* â€“ truy váº¥n trá»±c tiáº¿p ná»™i dung
  4. *Implied* â€“ cáº§n suy luáº­n ngá»¯ nghÄ©a
  5. *Unclear* â€“ mÆ¡ há»“, khÃ´ng rÃµ ngá»¯ cáº£nh

PhÆ°Æ¡ng phÃ¡p: vá»›i má»—i cÃ¢u há»i, truy há»“i **top-10 Ä‘oáº¡n gáº§n nháº¥t**, xem Ä‘oáº¡n gá»‘c cÃ³ xuáº¥t hiá»‡n khÃ´ng Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm chÃ­nh xÃ¡c.

---

## **3. Káº¿t quáº£ tá»•ng há»£p**

| MÃ´ hÃ¬nh embedding               | Accuracy tá»•ng thá»ƒ |      Short |       Long |     Direct |    Implied |    Unclear |
| ------------------------------- | ----------------: | ---------: | ---------: | ---------: | ---------: | ---------: |
| **mxbai-embed-large**           |            57.96% |     45.00% |     93.75% |     53.75% |     73.75% |     25.00% |
| **nomic-embed-text**            |            59.20% |     43.75% |     95.00% |     55.00% |     73.75% |     30.00% |
| **bge-m3**                      |        **63.93%** | **55.00%** | **95.00%** | **61.25%** | **80.00%** | **30.00%** |
| **qwen3-embedding:0.6b**        |            53.48% |     28.75% |     91.25% |     50.00% |     75.00% |     23.75% |
| **qwen3-embedding:4b**          |            57.46% |     37.50% |     96.25% |     55.00% |     77.50% |     22.50% |
| **embeddinggemma:300m**         |            29.60% |      7.50% |     68.75% |     28.75% |     36.25% |      7.50% |
| **snowflake-arctic-embed:335m** |            38.06% |     30.00% |     68.75% |     35.00% |     46.25% |     11.25% |
| **granite-embedding:278m**      |            59.70% |     48.75% |     93.75% |     52.50% |     77.50% |     27.50% |

---

## **4. PhÃ¢n tÃ­ch káº¿t quáº£**

### **4.1. Nháº­n xÃ©t tá»•ng thá»ƒ**

* **BGE-M3** Ä‘áº¡t hiá»‡u nÄƒng cao nháº¥t (63.93%), thá»ƒ hiá»‡n rÃµ kháº£ nÄƒng biá»ƒu diá»…n ngá»¯ nghÄ©a vÃ  khá»›p ngá»¯ cáº£nh tá»‘t.
  â†’ MÃ´ hÃ¬nh nÃ y sá»­ dá»¥ng kiáº¿n trÃºc *multi-vector dense embedding*, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn nhiá»u táº­p dá»¯ liá»‡u Ä‘á»‘i ngá»¯ nghÄ©a (contrastive datasets) nÃªn táº¡o ra khÃ´ng gian vector cÃ³ tÃ­nh phÃ¢n biá»‡t cao.

* **Nomic-embed-text** vÃ  **Granite-embedding** Ä‘áº¡t hiá»‡u nÄƒng á»•n Ä‘á»‹nh (~59â€“60%), pháº£n Ã¡nh thiáº¿t káº¿ cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a.
  â†’ Hai mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c tá»‘i Æ°u cho â€œgeneral text representationâ€, phÃ¹ há»£p cho truy váº¥n Ä‘a lÄ©nh vá»±c.

* **Qwen3 (0.6B vÃ  4B)** cho tháº¥y sá»± thiáº¿u á»•n Ä‘á»‹nh: máº·c dÃ¹ báº£n 4B lá»›n hÆ¡n, nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng vÆ°á»£t trá»™i.
  â†’ NguyÃªn nhÃ¢n lÃ  vÃ¬ Qwen3 táº­p trung huáº¥n luyá»‡n cho sinh ngÃ´n ngá»¯ (language modeling) hÆ¡n lÃ  embedding retrieval.

* **Gemma vÃ  Arctic** cÃ³ Ä‘á»™ chÃ­nh xÃ¡c ráº¥t tháº¥p, cho tháº¥y **embedding space cá»§a chÃºng khÃ´ng Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho semantic retrieval** â€” phÃ¹ há»£p hÆ¡n vá»›i inference hoáº·c nhiá»‡m vá»¥ phÃ¢n loáº¡i Ä‘Æ¡n giáº£n.

---

### **4.2. PhÃ¢n tÃ­ch theo loáº¡i cÃ¢u há»i**

| Loáº¡i cÃ¢u há»i | MÃ´ hÃ¬nh dáº«n Ä‘áº§u     | PhÃ¢n tÃ­ch                                                                                   |
| ------------ | ------------------- | ------------------------------------------------------------------------------------------- |
| **Short**    | BGE-M3 (55%)        | CÃ³ kháº£ nÄƒng encode tá»« vá»±ng ngáº¯n chÃ­nh xÃ¡c nhá» huáº¥n luyá»‡n Ä‘a táº§ng vá»›i contrastive loss.      |
| **Long**     | Qwen3-4B (96.25%)   | Lá»£i tháº¿ á»Ÿ biá»ƒu diá»…n ngá»¯ cáº£nh dÃ i, do dung lÆ°á»£ng model lá»›n giÃºp giá»¯ Ä‘Æ°á»£c thÃ´ng tin dÃ i háº¡n.  |
| **Direct**   | BGE-M3 (61.25%)     | Hiá»ƒu tá»‘t ná»™i dung xuáº¥t hiá»‡n rÃµ trong Ä‘oáº¡n vÄƒn; embedding Ä‘a chiá»u giÃºp phÃ¢n biá»‡t chÃ­nh xÃ¡c. |
| **Implied**  | BGE-M3 (80%)        | Máº¡nh nháº¥t vá» suy luáº­n ngá»¯ nghÄ©a â€“ káº¿t quáº£ cá»§a viá»‡c huáº¥n luyá»‡n trÃªn NLI/STSB.                |
| **Unclear**  | Nomic, BGE-M3 (30%) | DÃ¹ má»i mÃ´ hÃ¬nh Ä‘á»u yáº¿u á»Ÿ truy váº¥n mÆ¡ há»“, BGE-M3 vÃ  Nomic Ã­t bá»‹ nhiá»…u nháº¥t.                  |

---

### **4.3. Giáº£i thÃ­ch lÃ½ thuyáº¿t**

1. **Cáº¥u trÃºc huáº¥n luyá»‡n contrastive learning** (vÃ­ dá»¥ nhÆ° cá»§a BGE, Nomic) giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch phÃ¢n tÃ¡ch cÃ¡c vÄƒn báº£n tÆ°Æ¡ng tá»± vÃ  khÃ¡c biá»‡t trong khÃ´ng gian vector â†’ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c truy há»“i.
2. **Embedding dimension vÃ  sá»‘ lÆ°á»£ng cáº·p dá»¯ liá»‡u huáº¥n luyá»‡n** áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n kháº£ nÄƒng khá»›p ngá»¯ nghÄ©a. MÃ´ hÃ¬nh lá»›n nhÆ°ng khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘Ãºng cÃ¡ch (nhÆ° Qwen3-4B) sáº½ khÃ´ng tá»‘i Æ°u cho retrieval.
3. **CÃ¡c mÃ´ hÃ¬nh nhá»** cÃ³ khÃ´ng gian vector háº¹p hÆ¡n â†’ dá»… xáº£y ra hiá»‡n tÆ°á»£ng â€œsemantic collapseâ€, khiáº¿n cÃ¢u há»i tÆ°Æ¡ng tá»± khÃ³ Ä‘Æ°á»£c phÃ¢n biá»‡t.

---

### **4.4. Biá»ƒu Ä‘á»“ minh há»a**

![Grouped Column Chart](evaluation_data/grouped_column.png)

#### ğŸ”¹ *HÃ¬nh 4.1 â€“ Biá»ƒu Ä‘á»“ cá»™t nhÃ³m (Grouped Column Chart)*

Biá»ƒu Ä‘á»“ cho tháº¥y BGE-M3 ná»•i báº­t nháº¥t á»Ÿ táº¥t cáº£ nhÃ³m cÃ¢u há»i, Ä‘áº·c biá»‡t â€œImpliedâ€ vÃ  â€œDirectâ€.

---

## **5. Káº¿t luáº­n chÆ°Æ¡ng**

1. **BGE-M3** lÃ  mÃ´ hÃ¬nh **tá»‘i Æ°u nháº¥t** cho tÃ¡c vá»¥ RAG cá»¥c bá»™, Ä‘áº¡t hiá»‡u nÄƒng cao nháº¥t á»Ÿ cáº£ Ä‘á»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ vÃ  tá»«ng loáº¡i truy váº¥n.
2. **Nomic-embed-text** vÃ  **Granite-278M** lÃ  lá»±a chá»n nháº¹, thÃ­ch há»£p cho triá»ƒn khai ná»™i bá»™ vá»›i tÃ i nguyÃªn háº¡n cháº¿.
3. **Qwen3-4B** cÃ³ kháº£ nÄƒng xá»­ lÃ½ ngá»¯ cáº£nh dÃ i tá»‘t, nhÆ°ng chÆ°a vÆ°á»£t trá»™i do khÃ´ng tá»‘i Æ°u Ä‘áº·c thÃ¹ cho retrieval.
4. **Gemma vÃ  Arctic** chá»‰ nÃªn dÃ¹ng cho thá»­ nghiá»‡m hoáº·c mÃ´ phá»ng pipeline RAG.
5. Vá»›i cÃ¡c á»©ng dá»¥ng RAG an ninh máº¡ng, lá»±a chá»n tá»‘t nháº¥t lÃ  **BGE-M3** hoáº·c **Nomic-embed-text**, Ä‘áº£m báº£o cÃ¢n báº±ng giá»¯a hiá»‡u nÄƒng, tá»‘c Ä‘á»™ vÃ  tÃ­nh riÃªng tÆ° dá»¯ liá»‡u.

---

Báº¡n muá»‘n mÃ¬nh giÃºp Ä‘á»‹nh dáº¡ng chÆ°Æ¡ng nÃ y sang **máº«u luáº­n vÄƒn chuáº©n Word hoáº·c LaTeX** (vá»›i báº£ng, hÃ¬nh vÃ  chÃº thÃ­ch tá»± Ä‘á»™ng) Ä‘á»ƒ báº¡n chÃ¨n vÃ o bÃ¡o cÃ¡o khÃ´ng?
